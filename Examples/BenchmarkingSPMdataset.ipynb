{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y MLBenchmarks && pip install git+https://github.com/rcpsilva/MLBenchmarks@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from MLBenchmarks.benchmarking_methods  import load_regression_datasets, run_cross_dataset_benchmark_models, load_specific_datasets\n",
    "from MLBenchmarks.regression_datasets_loaders import load_spm_demagnetization_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running load_spm_demagnetization_FEM ...\n",
      "Running load_spm_demagnetization_analytical ...\n"
     ]
    }
   ],
   "source": [
    "datasets = load_specific_datasets(['load_spm_demagnetization_analytical','load_spm_demagnetization_FEM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 17.23214| train_mae: 20.41526| train_rmse: 28.74273| test_mae: 20.2621 | test_rmse: 28.85676|  0:00:00s\n",
      "epoch 1  | loss: 6.33205 | train_mae: 15.03458| train_rmse: 17.80155| test_mae: 15.00564| test_rmse: 17.66862|  0:00:01s\n",
      "epoch 2  | loss: 3.84964 | train_mae: 9.95161 | train_rmse: 11.63141| test_mae: 9.93508 | test_rmse: 11.59449|  0:00:01s\n",
      "epoch 3  | loss: 3.11514 | train_mae: 8.85134 | train_rmse: 10.51914| test_mae: 8.76839 | test_rmse: 10.4081 |  0:00:02s\n",
      "epoch 4  | loss: 2.44551 | train_mae: 8.07292 | train_rmse: 9.34721 | test_mae: 7.90293 | test_rmse: 9.16986 |  0:00:02s\n",
      "epoch 5  | loss: 2.29441 | train_mae: 5.95344 | train_rmse: 7.05739 | test_mae: 5.80124 | test_rmse: 6.86128 |  0:00:03s\n",
      "epoch 6  | loss: 1.94214 | train_mae: 6.05291 | train_rmse: 7.3489  | test_mae: 5.80998 | test_rmse: 7.03121 |  0:00:03s\n",
      "epoch 7  | loss: 2.07131 | train_mae: 3.9985  | train_rmse: 5.09701 | test_mae: 3.92312 | test_rmse: 5.05385 |  0:00:03s\n",
      "epoch 8  | loss: 1.70225 | train_mae: 3.89231 | train_rmse: 5.09213 | test_mae: 3.74415 | test_rmse: 4.89347 |  0:00:04s\n",
      "epoch 9  | loss: 1.50772 | train_mae: 3.26323 | train_rmse: 4.67287 | test_mae: 3.21331 | test_rmse: 4.57651 |  0:00:04s\n",
      "epoch 10 | loss: 1.1994  | train_mae: 2.59699 | train_rmse: 3.54529 | test_mae: 2.54113 | test_rmse: 3.55708 |  0:00:05s\n",
      "epoch 11 | loss: 1.1478  | train_mae: 2.55377 | train_rmse: 3.43819 | test_mae: 2.52589 | test_rmse: 3.52953 |  0:00:05s\n",
      "epoch 12 | loss: 0.98125 | train_mae: 2.00536 | train_rmse: 2.90109 | test_mae: 1.98237 | test_rmse: 3.02226 |  0:00:06s\n",
      "epoch 13 | loss: 1.00213 | train_mae: 1.88659 | train_rmse: 2.80631 | test_mae: 1.84443 | test_rmse: 2.842   |  0:00:06s\n",
      "epoch 14 | loss: 0.81432 | train_mae: 1.65706 | train_rmse: 2.5636  | test_mae: 1.64221 | test_rmse: 2.6841  |  0:00:07s\n",
      "epoch 15 | loss: 1.01272 | train_mae: 1.52437 | train_rmse: 2.31803 | test_mae: 1.52127 | test_rmse: 2.42552 |  0:00:07s\n",
      "epoch 16 | loss: 0.78763 | train_mae: 1.4643  | train_rmse: 2.02036 | test_mae: 1.46381 | test_rmse: 2.0848  |  0:00:07s\n",
      "epoch 17 | loss: 0.72948 | train_mae: 1.1431  | train_rmse: 1.6409  | test_mae: 1.17468 | test_rmse: 1.79166 |  0:00:08s\n",
      "epoch 18 | loss: 0.68253 | train_mae: 1.33944 | train_rmse: 1.8561  | test_mae: 1.34833 | test_rmse: 1.89064 |  0:00:08s\n",
      "epoch 19 | loss: 0.57068 | train_mae: 0.94515 | train_rmse: 1.38149 | test_mae: 0.95661 | test_rmse: 1.47097 |  0:00:09s\n",
      "epoch 20 | loss: 0.49225 | train_mae: 1.06564 | train_rmse: 1.54567 | test_mae: 1.07775 | test_rmse: 1.62623 |  0:00:09s\n",
      "epoch 21 | loss: 0.6322  | train_mae: 0.90658 | train_rmse: 1.26165 | test_mae: 0.91449 | test_rmse: 1.38543 |  0:00:10s\n",
      "epoch 22 | loss: 0.52455 | train_mae: 0.75565 | train_rmse: 1.07152 | test_mae: 0.76878 | test_rmse: 1.1367  |  0:00:10s\n",
      "epoch 23 | loss: 0.4361  | train_mae: 0.73337 | train_rmse: 1.04322 | test_mae: 0.72007 | test_rmse: 1.06309 |  0:00:11s\n",
      "epoch 24 | loss: 0.55119 | train_mae: 0.71831 | train_rmse: 1.0272  | test_mae: 0.75004 | test_rmse: 1.14674 |  0:00:11s\n",
      "epoch 25 | loss: 0.44537 | train_mae: 0.74863 | train_rmse: 1.06969 | test_mae: 0.75744 | test_rmse: 1.14567 |  0:00:12s\n",
      "epoch 26 | loss: 0.82893 | train_mae: 0.66749 | train_rmse: 0.99941 | test_mae: 0.70441 | test_rmse: 1.15593 |  0:00:12s\n",
      "epoch 27 | loss: 0.63843 | train_mae: 0.68765 | train_rmse: 0.91652 | test_mae: 0.73093 | test_rmse: 1.00904 |  0:00:12s\n",
      "epoch 28 | loss: 0.40688 | train_mae: 0.60274 | train_rmse: 0.95183 | test_mae: 0.63155 | test_rmse: 1.08139 |  0:00:13s\n",
      "epoch 29 | loss: 0.46984 | train_mae: 0.60812 | train_rmse: 0.78115 | test_mae: 0.63903 | test_rmse: 0.85639 |  0:00:13s\n",
      "epoch 30 | loss: 0.39261 | train_mae: 0.51412 | train_rmse: 0.76681 | test_mae: 0.53488 | test_rmse: 0.88544 |  0:00:14s\n",
      "epoch 31 | loss: 0.42689 | train_mae: 0.4811  | train_rmse: 0.71174 | test_mae: 0.4892  | test_rmse: 0.76916 |  0:00:14s\n",
      "epoch 32 | loss: 0.46418 | train_mae: 0.62163 | train_rmse: 1.03439 | test_mae: 0.65398 | test_rmse: 1.1776  |  0:00:15s\n",
      "epoch 33 | loss: 0.4259  | train_mae: 0.48374 | train_rmse: 0.66548 | test_mae: 0.5049  | test_rmse: 0.72315 |  0:00:15s\n",
      "epoch 34 | loss: 0.32745 | train_mae: 0.49203 | train_rmse: 0.73521 | test_mae: 0.51579 | test_rmse: 0.83249 |  0:00:16s\n",
      "epoch 35 | loss: 0.53462 | train_mae: 0.47875 | train_rmse: 0.63445 | test_mae: 0.48889 | test_rmse: 0.6719  |  0:00:16s\n",
      "epoch 36 | loss: 0.36483 | train_mae: 0.4755  | train_rmse: 0.71804 | test_mae: 0.48827 | test_rmse: 0.81142 |  0:00:17s\n",
      "epoch 37 | loss: 0.3232  | train_mae: 0.41437 | train_rmse: 0.58837 | test_mae: 0.42676 | test_rmse: 0.64859 |  0:00:17s\n",
      "epoch 38 | loss: 0.4231  | train_mae: 0.48414 | train_rmse: 0.67962 | test_mae: 0.49786 | test_rmse: 0.76148 |  0:00:18s\n",
      "epoch 39 | loss: 0.31759 | train_mae: 0.4559  | train_rmse: 0.70629 | test_mae: 0.47569 | test_rmse: 0.8207  |  0:00:18s\n",
      "epoch 40 | loss: 0.34891 | train_mae: 0.44945 | train_rmse: 0.68524 | test_mae: 0.46903 | test_rmse: 0.79688 |  0:00:18s\n",
      "epoch 41 | loss: 0.23952 | train_mae: 0.39131 | train_rmse: 0.52022 | test_mae: 0.4117  | test_rmse: 0.59793 |  0:00:19s\n",
      "epoch 42 | loss: 0.30611 | train_mae: 0.47704 | train_rmse: 0.72387 | test_mae: 0.50254 | test_rmse: 0.82944 |  0:00:19s\n",
      "epoch 43 | loss: 0.40977 | train_mae: 0.34931 | train_rmse: 0.47595 | test_mae: 0.36728 | test_rmse: 0.53852 |  0:00:20s\n",
      "epoch 44 | loss: 0.30802 | train_mae: 0.46452 | train_rmse: 0.6864  | test_mae: 0.48672 | test_rmse: 0.76993 |  0:00:20s\n",
      "epoch 45 | loss: 0.30298 | train_mae: 0.41809 | train_rmse: 0.57496 | test_mae: 0.41694 | test_rmse: 0.60597 |  0:00:21s\n",
      "epoch 46 | loss: 0.36348 | train_mae: 0.4065  | train_rmse: 0.57486 | test_mae: 0.43191 | test_rmse: 0.65899 |  0:00:21s\n",
      "epoch 47 | loss: 0.2225  | train_mae: 0.32467 | train_rmse: 0.43827 | test_mae: 0.34334 | test_rmse: 0.50391 |  0:00:22s\n",
      "epoch 48 | loss: 0.23476 | train_mae: 0.46641 | train_rmse: 0.69381 | test_mae: 0.49159 | test_rmse: 0.77773 |  0:00:23s\n",
      "epoch 49 | loss: 0.40617 | train_mae: 0.29456 | train_rmse: 0.42213 | test_mae: 0.3047  | test_rmse: 0.4717  |  0:00:23s\n",
      "epoch 50 | loss: 0.2199  | train_mae: 0.45459 | train_rmse: 0.67459 | test_mae: 0.48026 | test_rmse: 0.75903 |  0:00:24s\n",
      "epoch 51 | loss: 0.25516 | train_mae: 0.37585 | train_rmse: 0.53714 | test_mae: 0.37266 | test_rmse: 0.56113 |  0:00:24s\n",
      "epoch 52 | loss: 0.34234 | train_mae: 0.32189 | train_rmse: 0.46671 | test_mae: 0.32897 | test_rmse: 0.52275 |  0:00:25s\n",
      "epoch 53 | loss: 0.2645  | train_mae: 0.39573 | train_rmse: 0.55568 | test_mae: 0.41224 | test_rmse: 0.60908 |  0:00:25s\n",
      "epoch 54 | loss: 0.23943 | train_mae: 0.26876 | train_rmse: 0.37905 | test_mae: 0.28211 | test_rmse: 0.42086 |  0:00:26s\n",
      "epoch 55 | loss: 0.18434 | train_mae: 0.31223 | train_rmse: 0.43657 | test_mae: 0.32742 | test_rmse: 0.49358 |  0:00:26s\n",
      "epoch 56 | loss: 0.20431 | train_mae: 0.27904 | train_rmse: 0.39869 | test_mae: 0.29089 | test_rmse: 0.4525  |  0:00:27s\n",
      "epoch 57 | loss: 0.31132 | train_mae: 0.3476  | train_rmse: 0.42956 | test_mae: 0.36183 | test_rmse: 0.48009 |  0:00:27s\n",
      "epoch 58 | loss: 0.2771  | train_mae: 0.2759  | train_rmse: 0.38    | test_mae: 0.29    | test_rmse: 0.44599 |  0:00:28s\n",
      "epoch 59 | loss: 0.17873 | train_mae: 0.24135 | train_rmse: 0.35536 | test_mae: 0.24885 | test_rmse: 0.3943  |  0:00:28s\n",
      "epoch 60 | loss: 0.29876 | train_mae: 0.32044 | train_rmse: 0.4647  | test_mae: 0.33784 | test_rmse: 0.53852 |  0:00:29s\n",
      "epoch 61 | loss: 0.21584 | train_mae: 0.26688 | train_rmse: 0.37516 | test_mae: 0.27681 | test_rmse: 0.42392 |  0:00:29s\n",
      "epoch 62 | loss: 0.18188 | train_mae: 0.27711 | train_rmse: 0.40162 | test_mae: 0.28654 | test_rmse: 0.4561  |  0:00:30s\n",
      "epoch 63 | loss: 0.21822 | train_mae: 0.24581 | train_rmse: 0.35339 | test_mae: 0.253   | test_rmse: 0.40123 |  0:00:30s\n",
      "epoch 64 | loss: 0.13541 | train_mae: 0.26101 | train_rmse: 0.35361 | test_mae: 0.27543 | test_rmse: 0.40433 |  0:00:31s\n",
      "epoch 65 | loss: 0.23954 | train_mae: 0.29588 | train_rmse: 0.41909 | test_mae: 0.31215 | test_rmse: 0.48052 |  0:00:32s\n",
      "epoch 66 | loss: 0.18105 | train_mae: 0.22348 | train_rmse: 0.3101  | test_mae: 0.23407 | test_rmse: 0.35548 |  0:00:32s\n",
      "epoch 67 | loss: 0.25068 | train_mae: 0.23274 | train_rmse: 0.32334 | test_mae: 0.25038 | test_rmse: 0.37795 |  0:00:33s\n",
      "epoch 68 | loss: 0.15015 | train_mae: 0.25541 | train_rmse: 0.35615 | test_mae: 0.26163 | test_rmse: 0.3972  |  0:00:34s\n",
      "epoch 69 | loss: 0.12799 | train_mae: 0.20718 | train_rmse: 0.28734 | test_mae: 0.21361 | test_rmse: 0.32933 |  0:00:34s\n",
      "epoch 70 | loss: 0.2271  | train_mae: 0.25027 | train_rmse: 0.32466 | test_mae: 0.25817 | test_rmse: 0.35924 |  0:00:35s\n",
      "epoch 71 | loss: 0.20426 | train_mae: 0.25634 | train_rmse: 0.34951 | test_mae: 0.26221 | test_rmse: 0.39451 |  0:00:35s\n",
      "epoch 72 | loss: 0.17702 | train_mae: 0.21567 | train_rmse: 0.28867 | test_mae: 0.22744 | test_rmse: 0.33487 |  0:00:36s\n",
      "epoch 73 | loss: 0.17102 | train_mae: 0.27517 | train_rmse: 0.39106 | test_mae: 0.28404 | test_rmse: 0.44821 |  0:00:36s\n",
      "epoch 74 | loss: 0.14829 | train_mae: 0.19387 | train_rmse: 0.27214 | test_mae: 0.20063 | test_rmse: 0.31667 |  0:00:37s\n",
      "epoch 75 | loss: 0.14288 | train_mae: 0.25754 | train_rmse: 0.37144 | test_mae: 0.26508 | test_rmse: 0.4242  |  0:00:37s\n",
      "epoch 76 | loss: 0.13229 | train_mae: 0.22579 | train_rmse: 0.30028 | test_mae: 0.23725 | test_rmse: 0.33533 |  0:00:38s\n",
      "epoch 77 | loss: 0.13725 | train_mae: 0.24104 | train_rmse: 0.34396 | test_mae: 0.24925 | test_rmse: 0.38157 |  0:00:38s\n",
      "epoch 78 | loss: 0.21207 | train_mae: 0.17811 | train_rmse: 0.25169 | test_mae: 0.18532 | test_rmse: 0.28299 |  0:00:39s\n",
      "epoch 79 | loss: 0.1367  | train_mae: 0.27794 | train_rmse: 0.40204 | test_mae: 0.28901 | test_rmse: 0.44976 |  0:00:39s\n",
      "epoch 80 | loss: 0.14593 | train_mae: 0.177   | train_rmse: 0.2444  | test_mae: 0.18266 | test_rmse: 0.27405 |  0:00:40s\n",
      "epoch 81 | loss: 0.14859 | train_mae: 0.1633  | train_rmse: 0.22783 | test_mae: 0.17153 | test_rmse: 0.25999 |  0:00:40s\n",
      "epoch 82 | loss: 0.17659 | train_mae: 0.21886 | train_rmse: 0.30132 | test_mae: 0.23211 | test_rmse: 0.33991 |  0:00:41s\n",
      "epoch 83 | loss: 0.14368 | train_mae: 0.23677 | train_rmse: 0.32744 | test_mae: 0.248   | test_rmse: 0.38159 |  0:00:41s\n",
      "epoch 84 | loss: 0.20001 | train_mae: 0.25599 | train_rmse: 0.35106 | test_mae: 0.2663  | test_rmse: 0.39864 |  0:00:42s\n",
      "epoch 85 | loss: 0.23009 | train_mae: 0.2144  | train_rmse: 0.29282 | test_mae: 0.22586 | test_rmse: 0.34178 |  0:00:42s\n",
      "epoch 86 | loss: 0.40342 | train_mae: 0.26389 | train_rmse: 0.33718 | test_mae: 0.26743 | test_rmse: 0.34949 |  0:00:43s\n",
      "epoch 87 | loss: 0.20883 | train_mae: 0.36822 | train_rmse: 0.52166 | test_mae: 0.37169 | test_rmse: 0.56396 |  0:00:43s\n",
      "epoch 88 | loss: 0.19258 | train_mae: 0.21396 | train_rmse: 0.31464 | test_mae: 0.21258 | test_rmse: 0.33707 |  0:00:44s\n",
      "epoch 89 | loss: 0.29306 | train_mae: 0.20925 | train_rmse: 0.31783 | test_mae: 0.21752 | test_rmse: 0.35265 |  0:00:44s\n",
      "epoch 90 | loss: 0.18777 | train_mae: 0.19901 | train_rmse: 0.26263 | test_mae: 0.2044  | test_rmse: 0.2803  |  0:00:45s\n",
      "epoch 91 | loss: 0.14794 | train_mae: 0.19283 | train_rmse: 0.29505 | test_mae: 0.2003  | test_rmse: 0.33079 |  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 91 with best_epoch = 81 and best_test_rmse = 0.25999\n"
     ]
    }
   ],
   "source": [
    "# Create tabnet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define the model\n",
    "tabnet = TabNetRegressor(optimizer_fn=torch.optim.Adam,\n",
    "                       scheduler_params={\"step_size\":10, \n",
    "                                         \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                      )\n",
    "\n",
    "dataset = load_spm_demagnetization_analytical()\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "tabnet.fit(\n",
    "    X_train,y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    eval_name=['train', 'test'],\n",
    "    eval_metric=['mae','rmse'],\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'LR':LinearRegression(),\n",
    "          'DT':DecisionTreeRegressor(),\n",
    "          'RF':RandomForestRegressor(),\n",
    "          'GB':GradientBoostingRegressor(),\n",
    "          'HGB':HistGradientBoostingRegressor()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['neg_mean_absolute_percentage_error','neg_mean_absolute_error','neg_root_mean_squared_error','explained_variance'] # accepts scikit-learn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 14.98it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "100%|██████████| 2/2 [01:09<00:00, 34.51s/it]\n",
      "100%|██████████| 2/2 [00:33<00:00, 16.62s/it]\n",
      "100%|██████████| 2/2 [00:18<00:00,  9.40s/it]\n",
      "100%|██████████| 5/5 [02:02<00:00, 24.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "output_json = 'regression_benchmarks.json'\n",
    "res = run_cross_dataset_benchmark_models(models, datasets, metrics, output_json, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LR', 'DT', 'RF', 'GB', 'HGB']\n",
      "['load_spm_demagnetization_FEM', 'load_spm_demagnetization_analytical']\n",
      "['fit_time', 'score_time', 'test_neg_mean_absolute_percentage_error', 'test_neg_mean_absolute_error', 'test_neg_root_mean_squared_error', 'test_explained_variance', 'memory_usage(MB)']\n",
      "load_spm_demagnetization_FEM\n",
      "\ttest_neg_mean_absolute_error\n",
      "\t\t                                 LR:\t 0.631 \t +- 0.019\n",
      "\t\t                                 DT:\t 0.163 \t +- 0.157\n",
      "\t\t                                 RF:\t 0.185 \t +- 0.077\n",
      "\t\t                                 GB:\t 0.311 \t +- 0.023\n",
      "\t\t                                HGB:\t 0.225 \t +- 0.039\n",
      "load_spm_demagnetization_analytical\n",
      "\ttest_neg_mean_absolute_error\n",
      "\t\t                                 LR:\t 0.812 \t +- 0.021\n",
      "\t\t                                 DT:\t 0.258 \t +- 0.255\n",
      "\t\t                                 RF:\t 0.192 \t +- 0.085\n",
      "\t\t                                 GB:\t 0.239 \t +- 0.029\n",
      "\t\t                                HGB:\t 0.159 \t +- 0.036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "models = list(res.keys())\n",
    "datasets = list(res[models[0]].keys())\n",
    "metrics = list(res[models[0]][datasets[0]].keys())\n",
    "\n",
    "print(models)\n",
    "print(datasets)\n",
    "print(metrics)\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f'{dataset}')\n",
    "    for metric in metrics[3:4]: # assesing neg_mean_absolute_error\n",
    "        print(f'\\t{metric}')\n",
    "        for model in models:\n",
    "            print(f'\\t\\t{model:>35}:\\t {np.abs(np.mean(res[model][dataset][metric])):.3f} \\t +- {np.std(res[model][dataset][metric]):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
